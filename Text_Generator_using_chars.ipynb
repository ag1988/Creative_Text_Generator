{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Character Sequences using RNNs\n",
    "## Author: Ankit Gupta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# helper function for reading from a file\n",
    "\n",
    "def open_read(path):\n",
    "    t = open(path, errors='ignore')\n",
    "    has_period = False\n",
    "    lines = []\n",
    "    while True:\n",
    "        line = t.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        if line.strip():\n",
    "            lines.append(line.strip())\n",
    "            if '.' in line:\n",
    "                has_period = True\n",
    "    if has_period:\n",
    "        delimiter = ' '\n",
    "    else:\n",
    "        delimiter = '. '\n",
    "    return (delimiter.join(lines)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# .txt files 3913 total chars 21641381\n",
      "Harmonic Convergences You're right, Maxim's strong point is that it's totally unsentimental and ungenteel. It's a sendup of the old model, but in a different way than, say, Hustler was, and the difference (surprise) reflects the sexual culture of the '90s. With its belligerent grossness and misogyny, Hustler rebelled against the establishment men's mags' class condescension, the earnest philosophizing about the sexual revolution, the \"thinking men's sex bomb\" syndrome, at the same time that it was deliberately goading feminists. It came right out with the anger that the regular men's mags tried to hide. Maxim pokes fun at its progenitors but with considerable ironic affection. It's not angry. In fact, while its fondness for the most idiotic, juvenile humor knows no bounds, any strong emotion is taboo (unless you count horror at having your penis mangled)--that's part of the British influence, I guess. And feminism isn't an issue, at least not directly--partly because its basic ideas have been assimilated and are taken for granted, partly because politics in general and feminism in particular barely exist in the consciousness of Maxim's age group. (Whereas Gear, which retains certain elements of the old men's mag ethos, and the old hostility--and not incidentally runs somewhat longer articles--does worry about and argue with feminism, as in its recent piece on sexual repression in the military, in which it is argued, after a fashion, that 1) we can't suppress male soldiers' urge to rape and harass women without suppressing the urge to kill that's the military's reason for being, 2) sexual harassment crusades interfere with women's equality, 3) women soldiers aren't men's equals anyway, and 4) why do we need women in the military in the first place?) Another thing I've noticed is that the trajectory of the new men's magazines and that of contemporary women's magazines seem to be converging, at least compared with 20 years ago. There used to be an enormous gulf between the sense of entitlement to the good things of life, including women, that pervaded the men's mags and women's anxious obsession with self-improvement in order to be worthy of male attention. In young women's magazines now, Cosmo being the paradigm, there's a much more bluntly instrumental, \"male\" attitude toward getting sex of the quantity and quality desired. Beauty is as central a preoccupation as ever, if anything more so, but the preoccupation has much more of a narcissistic, self-pampering quality and less of a desperation to make up for some irrecoverable, primal imperfection. In the men's magazines, there's much more of a sense that if you want women to give you the time of day, you have to make some effort to find out what they want and give it to them. In the old model, if men needed advice on women or sex, they got it from a male expert like the \"Playboy Advisor.\" Now Maxim features advice from women on such matters as how not to give the wrong impression on a first date. On the other hand, men still don't like to express anxiety directly--I'd guess that part of the reason men's mags have gotten sillier is that they've seized on men's time-honored method of covering up insecurity by clowning around. Regarding your suggestion that these magazines give men a safe place to be together without their heterosexuality being questioned: That sounds like more of a hope (on their part) than a reality. Because it's ultimately self-doubt that really matters, no place can really be safe--hence the need to escalate the jokes. All the King's Men In the September Vanity Fair , profiler Marjorie Williams offers this tasty Larry King anecdote: \"One of the most oft told of the Larry Stories that circulate at CNN concerns his take-out order when he eats dinner at his desk, before the show. His favorite dish, from a local Chinese restaurant, is a chicken with cashews--without the fat-laden cashews. When the food arrives, a young intern or production assistant is charged with combing through it to remove any errant nuts; then he or she has to swaddle the chicken in paper towels and squeeze out any excess oil.\" A friend of Chatterbox's, who recalls his two-year tenure as King's Grease Monkey, says that Williams got the story right, but underplays how loopy the talk-show host is about his vittles. \"I drained Larry's meat for almost two years while working for the show. The intern/production assistant must open his cranberry juice and pour it over ice for him, just so . He also wants the packets of hot mustard opened for him and spread over the chicken. If he spots nuts or excess grease in his grub, the show is ruined! During commercial breaks, he will harp about the botched meal, unsettling the King crew and guest alike. The intern/production assistant can only dive for cover.\" The retired Grease Monkey says that the only time there was absolute calm on the set was when King went out to dinner.\n"
     ]
    }
   ],
   "source": [
    "# reading all the .txt files in the OANC corpus\n",
    "import os\n",
    "\n",
    "dirs = ['./datasets/text_generator/OANC/written_1/journal/slate/']\n",
    "texts = []\n",
    "files = []\n",
    "for direc in dirs:\n",
    "    for file in os.listdir(direc):\n",
    "        path = os.path.join(direc, file)\n",
    "        if os.path.isdir(path):\n",
    "            dirs.append(path)\n",
    "        elif path.endswith('.txt'):\n",
    "            texts.append(open_read(path))\n",
    "            files.append(path)\n",
    "\n",
    "total_chars = 0\n",
    "for t in texts:\n",
    "    total_chars += len(t)\n",
    "\n",
    "print('# .txt files', len(texts), 'total chars', total_chars)\n",
    "print(' '.join(texts[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21645293 ['H' 'a' 'r' ..., 'r' 'i' 's']\n"
     ]
    }
   ],
   "source": [
    "# using 10000 of the read files \n",
    "\n",
    "text = ' '.join(texts[:10000])\n",
    "text_list = np.array(list(text))\n",
    "print(len(text_list), text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\\t', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+',\n",
       "       ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8',\n",
       "       '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E',\n",
       "       'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R',\n",
       "       'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_',\n",
       "       '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
       "       'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y',\n",
       "       'z', '{', '}'],\n",
       "      dtype='<U1')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e' 's' ' ' 'Y' 'o']\n",
      "[[0 0 1 0 0]\n",
      " [0 0 0 0 1]\n",
      " [1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 1 0]]\n",
      "94\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "binarizer = LabelBinarizer()\n",
    "print(text_list[19:24])\n",
    "print(binarizer.fit_transform(text_list[19:24])) # see last two rows\n",
    "\n",
    "# fitting the full dataset\n",
    "binarizer.fit(text_list) \n",
    "print(len(binarizer.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e' 's' ' ' 'Y' 'o']\n",
      "[2 4 0 1 3]\n",
      "94\n"
     ]
    }
   ],
   "source": [
    "# encoding the data\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "print(text_list[19:24])\n",
    "print(encoder.fit_transform(text_list[19:24])) #\n",
    "encoder.fit(text_list)\n",
    "print(len(encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 100, 94) (3, 100)\n",
      "73 79 84 80 79 1 68 80 79 85 70 79 69 84 1 85 73 66 85 1 36 77 74 79 85 80 79 1 74 84 1 85 73 70 1 3 78 80 84 85 1 69 74 84 83 70 81 86 85 66 67 77 70 1 81 83 70 84 74 69 70 79 85 1 70 87 70 83 15 3 1 53 73 70 1 81 83 80 71 86 84 74 80 79 1 80 71 1 84 68 66 79 69 66 77 84 14 14 71 83 \n",
      "79 84 80 79 1 68 80 79 85 70 79 69 84 1 85 73 66 85 1 36 77 74 79 85 80 79 1 74 84 1 85 73 70 1 3 78 80 84 85 1 69 74 84 83 70 81 86 85 66 67 77 70 1 81 83 70 84 74 69 70 79 85 1 70 87 70 83 15 3 1 53 73 70 1 81 83 80 71 86 84 74 80 79 1 80 71 1 84 68 66 79 69 66 77 84 14 14 71 83 80 "
     ]
    }
   ],
   "source": [
    "# preparing a big dataset - not very efficient to make a big dataset at once\n",
    "n_steps = 100\n",
    "\n",
    "def get_next_batch(epoch, batch, batch_size):\n",
    "    np.random.seed(epoch)\n",
    "    perm = np.random.permutation(len(text_list) - n_steps - 1) # starting indices of the sequnces of length n_steps + 1\n",
    "    starts_indices = perm[batch*batch_size: (batch + 1)*batch_size]\n",
    "    X_ret, y_ret = np.zeros( (batch_size, n_steps, len(binarizer.classes_)) ), np.zeros((batch_size, n_steps))\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        start = starts_indices[i]\n",
    "        X_ret[i], y_ret[i] = binarizer.transform(text_list[start: start + n_steps]), encoder.transform(text_list[start + 1: start + n_steps + 1])\n",
    "    \n",
    "    return X_ret.astype('int32'), y_ret.astype('int32')\n",
    "\n",
    "X_temp, y_temp = get_next_batch(0, 0, 3)\n",
    "print(X_temp.shape, y_temp.shape)\n",
    "\n",
    "for i in range(len(y_temp[0])):\n",
    "    print(np.argmax(X_temp[0, i, :]), end=' ')\n",
    "print()\n",
    "for i in range(len(y_temp[0])):\n",
    "    print(y_temp[0, i], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"rnn/transpose:0\", shape=(?, 100, 94), dtype=float32)\n",
      "(?, 100) ()\n"
     ]
    }
   ],
   "source": [
    "# constructing the RNN\n",
    "\n",
    "n_inputs = len(binarizer.classes_)\n",
    "n_outputs = n_inputs\n",
    "n_neurons = 200\n",
    "n_layers = 3\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(dtype='int32', shape=(None, n_steps, n_inputs), name='X') # one hot encoding of chars\n",
    "y = tf.placeholder(dtype='int32', shape=(None, n_steps), name='y')  # integer labels of chars\n",
    "\n",
    "layers = [tf.contrib.rnn.GRUCell(num_units=n_neurons, activation=tf.nn.tanh) for i in range(n_layers)]\n",
    "multi_cell = tf.contrib.rnn.MultiRNNCell(layers)\n",
    "# convert outputs of len n_neurons to n_outputs\n",
    "multi_cell_opw = tf.contrib.rnn.OutputProjectionWrapper(multi_cell, output_size=n_outputs)\n",
    "logits, states = tf.nn.dynamic_rnn(multi_cell_opw, tf.cast(X, dtype='float32'), dtype='float32')\n",
    "print(logits)\n",
    "\n",
    "# convert the output at each time step into probs using softmax \n",
    "probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "#loss\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(xentropy, name='loss')\n",
    "print(xentropy.shape, loss.shape)\n",
    "\n",
    "#optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "# saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# running a tf session for training\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "init.run()\n",
    "epoch, batch = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1500 loss: 1.45361  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-428ae6a2f9af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_steps\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0meval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 600\n",
    "\n",
    "while batch < (len(text_list) - n_steps - 1) // batch_size:\n",
    "    X_batch, y_batch = get_next_batch(epoch, batch, batch_size)\n",
    "    sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "    if batch % 50 == 0:\n",
    "        eval_loss = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        print('batch', batch, 'loss:', eval_loss, end='  ')\n",
    "        #if eval_loss < 1.7:\n",
    "        #    saver.save(sess, './datasets/savers/text_generator_chars/text_generator_chars')\n",
    "    batch += 1\n",
    "\n",
    "# loss should be < 1.6 for it to produce meaningfull words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.48638\n"
     ]
    }
   ],
   "source": [
    "# final loss\n",
    "\n",
    "print('loss:', loss.eval(feed_dict={X: X_batch, y: y_batch}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./datasets/savers/text_generator_chars/text_generator_chars'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving the trained model\n",
    "\n",
    "saver.save(sess, './datasets/savers/text_generator_chars/text_generator_chars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The New York Times , but we contract with the state in the story. The NYT and Texas, the New Yorker Can and New York Times , the New York Times , because the case of the secret off the party first, the president as the country, the start of the same actually are sent of any services are an endorsemory and to see their sense of the conservation of the second counter of their companience. It was a campaign to the present of the president. The New York Times lines to take a standing to the case to an antile that the state in the same touring to be a story is that his case offer and self-and tripping only be a street concern that they've a case of the same anyone who was the complete change of a care on the party of controssed to be able out on the same time in the same titled and the characterial considerable to the president was a contraction. The NYT and Newsweeks without a party on this in a contribution of the submit that the second second improval on this way of cannal to the contest\n"
     ]
    }
   ],
   "source": [
    "# generating new sequence from the model\n",
    "\n",
    "starting_context = 'The '\n",
    "\n",
    "seed = list(starting_context)\n",
    "output_length = 1000\n",
    "top_k = 3\n",
    "output_seq = seed[:]\n",
    "\n",
    "while(len(output_seq) < output_length):\n",
    "    # if needed pad the seed to make its length equal to n_steps \n",
    "    seed_padded = seed[:]\n",
    "    for i in range(n_steps - len(seed)):\n",
    "        seed_padded.append(' ')\n",
    "    \n",
    "    # reshape the padded seed \n",
    "    X_seed = np.zeros( (1, n_steps, len(binarizer.classes_)) )\n",
    "    X_seed[0] = binarizer.transform(seed_padded)\n",
    "    X_seed = X_seed.astype('int32')\n",
    "    \n",
    "    seed_probs = probs.eval(feed_dict={X: X_seed})[0]\n",
    "    next_char_prob_distr = seed_probs[len(seed)-1]\n",
    "    # to make sure we dont pick a character that has too small prob\n",
    "    # we only keep top top_k chars\n",
    "    \n",
    "    top_k_probs = sorted(next_char_prob_distr)[-top_k:]\n",
    "    for i in range(len(binarizer.classes_)):\n",
    "        if next_char_prob_distr[i] < top_k_probs[0]:\n",
    "            next_char_prob_distr[i] = 0\n",
    "    # normalize\n",
    "    next_char_prob_distr /= np.sum(next_char_prob_distr)\n",
    "    \n",
    "    # sample the next char label from this distribution\n",
    "    next_char_label = np.random.choice(len(binarizer.classes_), p=next_char_prob_distr)\n",
    "    next_char = encoder.inverse_transform(next_char_label)\n",
    "    output_seq.append(next_char)\n",
    "    \n",
    "    # update seed\n",
    "    seed.append(next_char)\n",
    "    if len(seed) > n_steps:\n",
    "        seed = seed[1:]\n",
    "        \n",
    "print(''.join(output_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
